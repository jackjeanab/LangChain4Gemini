{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCiCklZjuA1mfcRhiMURTm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackjeanab/LangChain4Gemini/blob/main/4_6%E5%AF%A6%E4%BD%9CConditionalChain%E6%A9%9F%E5%88%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  langchain-google-genai"
      ],
      "metadata": {
        "id": "ReaTKeK2ngJ6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 引入Chain模組\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# 引入Gemini LLM模組\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# 引入prompt模組\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "3bNXIx1yFkbq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Initialize the Gemini Pro model\n",
        "# Use \"gemini-1.5-pro-latest\" or \"gemini-1.5-flash-latest\" model for better instruction following\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro-latest\",\n",
        "    temperature=0.5)"
      ],
      "metadata": {
        "id": "FPqGk6tehmQe"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定義情緒分析的提示樣板\n",
        "sentiment_analysis_prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    template=\"根據這段話分析情緒，並僅回答 'positive' 或 'negative'：'{user_input}'\"\n",
        ")\n",
        "# 建立情緒分析的 LLMChain\n",
        "sentiment_analysis_chain = LLMChain(llm=llm, prompt=sentiment_analysis_prompt)"
      ],
      "metadata": {
        "id": "Et5rOdXaGI48"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 負面情緒應對的 PromptTemplate\n",
        "negative_response_prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    template=\"使用者說了這段話：'{user_input}'。請給出一段安撫的回應。\"\n",
        ")\n",
        "negative_response_chain = LLMChain(llm=llm, prompt=negative_response_prompt)"
      ],
      "metadata": {
        "id": "LeIQh51BGTag"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 正面情緒應對的 PromptTemplate\n",
        "positive_response_prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    template=\"使用者說了這段話：'{user_input}'。請給出一段正向互動的回應。\"\n",
        ")\n",
        "positive_response_chain = LLMChain(llm=llm, prompt=positive_response_prompt)"
      ],
      "metadata": {
        "id": "lNu6EL9aGaGg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_conditional_chain(user_input):\n",
        "    # 第一步：使用 LLM 來分析情緒\n",
        "    sentiment_result = sentiment_analysis_chain.run({\"user_input\": user_input})\n",
        "\n",
        "    # 第二步：根據情緒結果選擇要執行的chain\n",
        "    if sentiment_result.strip().lower() == \"negative\":\n",
        "        # 如果情緒為負面，執行負面應對chain\n",
        "        return negative_response_chain.invoke({\"user_input\": user_input})\n",
        "    else:\n",
        "        # 如果情緒為正面，執行正面應對鏈結\n",
        "        return positive_response_chain.invoke({\"user_input\": user_input})"
      ],
      "metadata": {
        "id": "wdIZJl_UGfHk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 執行 Conditional Chain\n",
        "result = execute_conditional_chain(\"我對於你們的服務感到普通，服務人員表現普通。\")\n",
        "\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v2rh1qqGjyq",
        "outputId": "27e7020d-ae31-4e51-e2e4-491a09ec3659"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "謝謝您提供寶貴的意見。我們很重視您的回饋，並對於您這次的體驗感到抱歉，似乎沒有達到您的期望。我們理解「普通」代表還有進步空間，希望能更了解哪些方面讓您覺得不夠滿意。  方便的話，是否能提供更多細節，例如哪些服務環節或服務人員的哪些方面讓您覺得普通？您的意見將有助於我們改進服務品質，提供更好的體驗。  我們期盼未來有機會再次為您服務，並展現我們提升後的服務。\n"
          ]
        }
      ]
    }
  ]
}