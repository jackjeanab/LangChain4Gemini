{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcM+w2mgYGH/l0IyI6kK1+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackjeanab/LangChain4Gemini/blob/main/4_6%E5%AF%A6%E4%BD%9CConditionalChain%E6%A9%9F%E5%88%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  langchain-google-genai"
      ],
      "metadata": {
        "id": "ReaTKeK2ngJ6"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 引入Chain模組\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# 引入Gemini LLM模組\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# 引入prompt模組\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "3bNXIx1yFkbq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Initialize the Gemini Pro model\n",
        "# Use \"gemini-2.0-flash-exp\"\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.5)"
      ],
      "metadata": {
        "id": "FPqGk6tehmQe"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定義情緒分析的提示樣板\n",
        "sentiment_analysis_prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    template=\"根據這段話分析情緒，並僅回答 'positive' 或 'negative'：'{user_input}'\"\n",
        ")\n",
        "# 建立情緒分析的 LLMChain\n",
        "sentiment_analysis_chain = LLMChain(llm=llm, prompt=sentiment_analysis_prompt)"
      ],
      "metadata": {
        "id": "Et5rOdXaGI48"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 負面情緒應對的 PromptTemplate\n",
        "negative_response_prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    template=\"使用者說了這段話：'{user_input}'。請給出一段安撫的回應。\"\n",
        ")\n",
        "negative_response_chain = LLMChain(llm=llm, prompt=negative_response_prompt)"
      ],
      "metadata": {
        "id": "LeIQh51BGTag"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 正面情緒應對的 PromptTemplate\n",
        "positive_response_prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    template=\"使用者說了這段話：'{user_input}'。請給出一段正向互動的回應。\"\n",
        ")\n",
        "positive_response_chain = LLMChain(llm=llm, prompt=positive_response_prompt)"
      ],
      "metadata": {
        "id": "lNu6EL9aGaGg"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_conditional_chain(user_input):\n",
        "    # 第一步：使用 LLM 來分析情緒\n",
        "    sentiment_result = sentiment_analysis_chain.run({\"user_input\": user_input})\n",
        "\n",
        "    # 第二步：根據情緒結果選擇要執行的chain\n",
        "    if sentiment_result.strip().lower() == \"negative\":\n",
        "        # 如果情緒為負面，執行負面應對chain\n",
        "        return negative_response_chain.invoke({\"user_input\": user_input})\n",
        "    else:\n",
        "        # 如果情緒為正面，執行正面應對鏈結\n",
        "        return positive_response_chain.invoke({\"user_input\": user_input})"
      ],
      "metadata": {
        "id": "wdIZJl_UGfHk"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 執行 Conditional Chain\n",
        "result = execute_conditional_chain(\"我對於你們的服務感到普通，服務人員表現普通。\")\n",
        "\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v2rh1qqGjyq",
        "outputId": "76e0af37-4473-41e9-f607-c3323ac84860"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "好的，對於您覺得我們的服務表現普通，我感到非常抱歉。我們非常重視每一位顧客的體驗，聽到您這樣的回饋，我們感到有些失望。\n",
            "\n",
            "您的意見對我們來說非常重要，這能幫助我們不斷改進。我想了解更多關於您覺得「普通」的具體細節，例如：\n",
            "\n",
            "*   **在哪些方面您覺得我們的服務表現普通？** (例如：回覆速度、服務態度、解決問題的能力等等)\n",
            "*   **您期望的服務體驗是什麼樣的？**\n",
            "\n",
            "如果您願意分享更多資訊，我們會非常感激。我們將會仔細檢視您的意見，並努力提升我們的服務品質。\n",
            "\n",
            "同時，我想讓您知道，我們一直致力於提供更好的服務。我們希望未來能有機會讓您感受到我們的進步，並讓您對我們的服務感到滿意。\n",
            "\n",
            "再次感謝您的回饋，您的意見對我們非常寶貴。\n"
          ]
        }
      ]
    }
  ]
}